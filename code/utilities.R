## file containing utility functions
library(parallel)

## assign risk class based on risk probability (x)
## x: vector of risk probability (x <0.5 are "low" risk)
## cut1: probability greater than cut1 = "high" risk
## cut2: probablity greater than cut = "extreme" risk
assignRisk <- function(x, cut1=0.5, cut2=0.6) {
  outie <- rep("low", length(x))
  outie[x >= cut1] <- "high"
  outie[x >= cut2] <- "extreme"
  return(outie)
}


## generate risk prediction with elastic net model
## x: data.frame of program activity. each row is a subject
## cols: program labels (1 to 141) that match the columns of x
## model: elastic net risk model
##
## returns risk probability for each row of x
doPred <- function(x, cols, model=elastic_net) {
  input <- matrix(x, nrow=1)
  colnames(input) <- cols
  return( predict(model, input, type="prob")[,1])
}

## calculate Z score
doZ <- function(x) {return( (x - mean(x, na.rm=TRUE))/sd(x, na.rm=TRUE))}

## mat <- dataframe with true=true risk and pred= predicted risk
getFullRates <- function(mat) {
  
  getSum <- function(x, labs=c("extreme", "high", "low")) {
    out <- NULL
    for(i in 1:length(labs)) {
      out <- c(out, sum(x==labs[i]))
    }
    out <- c(length(x), signif(out/length(x)*100,3))
    return(out)
  }
  
  x_dat <- mat[mat$pred=="extreme",]
  if(nrow(x_dat) == 0) {
    x_tab <- rep(0, 4)
  } else {
     x_tab <- getSum(x_dat$true)
  }

  xh_dat <- mat[mat$pred=="extreme" | mat$pred=="high",]
  if(nrow(xh_dat) == 0) {
    xh_tab <- rep(0, 4)
  } else {
     xh_tab <-getSum(xh_dat$true)
  }
  
  xl_dat <- mat[mat$pred=="low",]
  if(nrow(xl_dat) == 0) {
    xl_tab <- rep(0, 4)
  } else {
    xl_tab <- getSum(xl_dat$true)
  }
  
  outie <- rbind(x_tab, xh_tab, xl_tab)
  outie <- data.frame(predicted=c("extreme", "high", "low"), outie)
  colnames(outie) <- c("SYGNAL", "total", "extreme", "high", "low")
  rownames(outie) <- NULL
  
  return(outie)
}


## get summary info for all subjects in subtype
getSubRates <- function(pheno=risk_sub) {
  
  ## risk percentage
  tabr <- data.frame(table(pheno$risk))
  tabr$perc <- signif((tabr$Freq/sum(tabr$Freq))*100, 3)
  
  ##  get median pfs
  survy <- survfit(Surv(time=PFS, event=D_PFS_FLAG)~ risk, data=pheno)
  mpfs <- summary(survy)$table[,7]
  tabr$mpfs <- mpfs
  
  ## get relapse percentages
  relaps <- tapply(pheno$D_PFS_FLAG, pheno$risk, sum)
  relaps_perc <- round((relaps/tabr$Freq)*100)
  tabr$relapse <- relaps_perc
  return(tabr)
}

## Get network activity for each gene in a program
## bkgd - activity of genes (generated by generateProgramActivity)
## programs - list of regulons for each program
## regulons - list of genes for each regulon
getBkgdByProgram <- function(bkgd, programs, regulons) {
  
  getActiveGenes <- function(pro, bkgd, programs, regulons) {
    regs <- programs[[pro]]
    genes <- unique(unlist(regulons[regs]))
    p_bak <- bkgd[genes,]
    df <- data.frame(program=pro, gene=rownames(p_bak), p_bak)
    rownames(df) <- NULL
    return(df)      
  }
  
  all_ret <- NULL
  for(pro in names(programs)) {
    ret <- getActiveGenes(pro, bkgd, programs, regulons)
    all_ret <- rbind(all_ret, ret)
  }
  
  return(all_ret)
} ## end getBkgdByProgram

## pred - vector of predicted risk
## truth - vector of true risk
getConfusionMatrix <- function(pred, truth) { 
  conf <- confusionMatrix(factor(pred), factor(truth))
  conf_out <- data.frame(conf$table)
  conf_mat <- matrix(conf_out$Freq, nrow=3, byrow=FALSE)
  TPR <- c(conf_mat[1,1]/sum(conf_mat[,1]), 
           conf_mat[2,2]/sum(conf_mat[,2]), 
           conf_mat[3,3]/sum(conf_mat[,3]))
  TPR[!is.finite(TPR)] <- 0
  FPR <- c(sum(conf_mat[1,c(2,3)])/sum(conf_mat[1,]), 
           sum(conf_mat[2,c(1,3)])/sum(conf_mat[2,]), 
           sum(conf_mat[3,1:2])/sum(conf_mat[3,]), 0)
  FPR[!is.finite(FPR)] <- 0
  conf_mat <- rbind(conf_mat, signif(TPR*100, 2))
  conf_mat <- data.frame(risk=c("extreme", "high", "low", "TPR%"),
                         cbind(conf_mat, signif(FPR*100, 1)))
  colnames(conf_mat) <- c("risk", "extreme", "high", "low", "FPR%")
  return(conf_mat)
}

## calculate ROCs for sky92 and gep panels
## pan_mat - table of gene expression data, row names are ensembl ids
## risky - vector of risk labels
## dat_lab - label for data set
## panny - sky92 and gep70 panel definitions
panelROC <- function(pan_mat, risky, dat_lab, panny=panel) {
  risky[risky=="extreme"] <- "high"
  risky <- factor(risky, levels=c("low", "high"), ordered=TRUE)
  sky_ind <- panny$panel == "sky"
  sky_dat <- pan_mat[panny$Preferred_Name[sky_ind],]
  sky_mean <- apply(sky_dat, 2, mean, na.rm=TRUE)
  
  gep_dat_up <- pan_mat[!sky_ind & panny$direction=="up",]
  gep_dat_down <- pan_mat[!sky_ind & panny$direction=="down",]
  gep_mean_up <-apply(gep_dat_up, 2, mean, na.rm=TRUE)
  gep_mean_down <- apply(gep_dat_down, 2, mean, na.rm=TRUE)
  
  ## data is Z score so assumption is that down_regulated genes are negative so sum them
  gep_mean <- gep_mean_up + gep_mean_down
  
  sky_roc <- roc(response=risky, predictor=sky_mean, auc=TRUE,
                 levels=unique(risky))
  gep_roc <- roc(response=risky, predictor=gep_mean, auc=TRUE,
                 levels=unique(risky))
  
  sky_out <- data.frame(data=dat_lab, panel="sky92", FPR=rev(1-sky_roc$specificities),
                        TPR=rev(sky_roc$sensitivities), AUC=signif(auc(sky_roc)[1], 4))
  gep_out <- data.frame(data=dat_lab, panel="gep70", FPR=rev(1-gep_roc$specificities),
                        TPR=rev(gep_roc$sensitivities), AUC=signif(auc(gep_roc)[1],4))
  return(rbind(sky_out, gep_out))
}


## mod - table of data from getConfusionMatrix
getRiskSummary <- function(mod) {
  
  xh_sum <- apply(mod[1:3,2:3], 1, sum)
  xh_sum2 <- apply(mod[1:2,2:4], 2, sum)
  
  x_perc <- c(mod$extreme[4], mod[1,5])
  xh_perc <- c(signif((sum(xh_sum[1:2])/sum(xh_sum))*100, 2),
               signif((xh_sum2[3]/sum(xh_sum))*100, 2))
  l_perc <- c(mod$low[4], mod[3,5])
  out <- rbind(x_perc, xh_perc, l_perc)
  dimnames(out) <- list(c("extreme", "high/extreme", "low"), c("TPR", "FPR"))               
  return(out)
}


### bootstrap data keeping risk proportions
## x = iteration
## risk_mat - phenotype data.frame
## prog_mat - program activity data.frame
doRiskIter <- function(x, risk_mat1=risk_sub, prog_mat1=prog_sub, do_porp=TRUE) {
  
  ## sample proportionally
  if(do_porp) {
    x_ind <- risk_mat1$risk=="extreme"
    x_risk <- sample(risk_mat1$sample[x_ind], sum(x_ind), replace=TRUE)
    h_ind <- risk_mat1$risk=="high"
    h_risk <- sample(risk_mat1$sample[h_ind], sum(h_ind), replace=TRUE)
    l_ind <- risk_mat1$risk=="low"
    l_risk <- sample(risk_mat1$sample[l_ind], sum(l_ind), replace=TRUE)
    
    ## set new tables and account for non-duplicate column names
    new_samp <- data.frame(sample=c(x_risk, h_risk, l_risk))
  } else {
    x_ind <- sample(risk_mat1$sample, replace=TRUE)
    new_samp <- data.frame(sample=x_ind)
  }
  new_risk <- dplyr::left_join(new_samp, risk_mat1, by="sample")
  new_prog <- prog_mat1[, new_risk$sample]
  new_risk$sample <- colnames(new_prog)
  
  ret <- getRanking(new_risk, new_prog)
  
  if((x %% 100) == 0) {
    print(paste("finished iteration", x))
  }
  return(ret)
} 

## run signicance test on program activity vs risk
## prog_dat - table of program network activity
## test - which test to run (cox, extreme_risk (extreme, high, low), 
##        risk (high, low))
## pheno - table of phenotype information
## n - number of top risk tables to output
runProgTest <- function(prog_dat, test="extreme_risk", pheno=mm_risk,
                        n=5, do_risk_perc=TRUE) {
  
  if(test=="cox") {
      pvals <- apply(prog_dat, 1, myCoxPh, risk=pheno)
  } else if(test=="extreme_risk") {
     pvals <- apply(prog_dat, 1, calcRisk, risky=pheno$extreme_risk)
  } else if(test=="risk") {
     pvals <- apply(prog_dat, 1, calcRisk, risky=pheno$risk)
  } else {
    stop(paste0(test, "not implemented"))
  }
  
  names(pvals) <- rownames(prog_dat)
  pval_sort <- sort(pvals)
  fdrs <- p.adjust(pval_sort, method="fdr")
  
  for(i in 1:n) {
    ##print(i)
    name <- names(pval_sort[i])
    fdr1 <- fdrs[i]
    net <- unlist(prog_dat[name,])
    tab <- getTableSimple(mat=net, risky=pheno$extreme_risk)
    title <- paste(name, "fdr:", fdr1)
    
    ## calculate percentage of high extreme risk patients
    if(do_risk_perc) {
      summs <- apply(tab[, 2:3], 1, sum)
      perc_risk <- signif((summs/summs[1])*100, 2)
      tab <- cbind(tab, perc_risk)
    }
    print(kable(tab, format="simple", caption=title))
  }
  
  return(data.frame(pvals=pval_sort, fdrs=fdrs))
} ## end runProgTest

## prune networks by cox hazard ratio for all subjects. For use in sapply function 
## order of x must match order of risk$sample
## x - row of data.frame of network activity for a regulon
## risk - table of risk meta_data with column labeles risk (low, high)
myCoxPh <- function(x, risk=mm_risk) {
  risk$net <- x
  if(var(x) == 0) {return(1)}
  res <- coxph(Surv(D_PFS, D_PFS_FLAG) ~ x, data=risk)
  return(summary(res)$coefficients[5])
}

## rank programs by number of extreme risk subjects that are separated
getRiskRank <-function(risk_tmp, prog_tmp, cutt=x_cut) {
  
  ret <- list()
  nameys <- rownames(prog_tmp)
  for(i in 1:nrow(prog_tmp)) {
    progg <- unlist(prog_tmp[i,])
    act <- sum(risk_tmp$risk[progg==1] == "extreme")
    inact <- sum(risk_tmp$risk[progg==-1] == "extreme")
  
    if(act > cutt) {
        ret[[nameys[i]]] <- c(1, act)  
    } else if (inact > cutt) {
      ret[[nameys[i]]] <- c(-1, inact)  
    }
  }
  
  if(length(ret) == 0) {
    out <- NULL
  } else {
    out <- data.frame(do.call(rbind, ret))
    colnames(out) <- c("activity", "num_ext")
    out$program <- rownames(out)
  }
  return(out)
}

## search for combination of two programs that separates extreme risk subjects
## risk_mat - phenotype data.frame
## prog_mat - program activity data.frame
## perc_cut - cutoff to prune results - final node must have less than perc_cut % low_risk subjects
## pval_cut - cutoff to prune results - final node must have cox pval less than pval_cut
getRanking <- function(risk_mat=risk_sub, prog_mat=prog_sub, risk_cut=0.40, perc_cut=50, pval_cut=0.01) {
  
  x_num <- sum(risk_mat$SYGNAL_risk=="extreme")
  x_cut <- round((x_num)*risk_cut)
  
  ## get top ranked programs on separating extreme risk subjects
  rank1 <- getRiskRank(risk_mat, prog_sub, cutt=x_cut)
  
  ## store results
  outie <- list()
  
  ## iterate through first node
  for(pro1 in rank1$program) {
    pro_ind <- which(rownames(prog_mat) == pro1)
    prog1 <- prog_mat[-pro_ind,]             ## remove pro from network table
    pro_act1 <- unlist(prog_mat[pro1,])        ## get pro's network activity
    pro_net1 <- rank1[pro1, "activity"]      ## get mode of activity (-1,1)
    
    ## select subjects who show the correct program activity for pro
    indy <- unlist(pro_act1 == pro_net1)
    if(sum(indy) < 2) {
      next()
    }
    prog_new <- prog1[,indy]
    risk_new <- risk_mat[indy,]
    
    ## find second program that separates extreme risk subjects (less stringent on second node)
    rank2 <- getRiskRank(risk_new, prog_new, cutt=(x_cut-1))
    
    ## no good second programs so skip
    if(is.null(rank2)) {
      next()
    }
    
    cox_mat <- risk_mat[,c("D_PFS", "D_PFS_FLAG", "SYGNAL_risk")]
    
    ## iterate through second node
    for(pro2 in rank2$program) {
      ## network activity
      pro_tmp <- unlist(prog_new[pro2,])
      
      pro_net2 <- rank2[pro2, "activity"]      ## get mode of activity (-1,1)
      
      ## get percentage of low risk subjects in second program
      low_perc <- round(sum(risk_new$SYGNAL_risk[pro_tmp==pro_net2] == "low")/length(pro_tmp)*100)
      
      ## get number of extreme risk subjects in second program
      high_perc <- round(sum(risk_new$SYGNAL_risk[pro_tmp==pro_net2] == "extreme")/x_num*100)
  
      if(low_perc < perc_cut) {
        pro_act2 <- unlist(prog_mat[pro2,])
        cox_tmp <- data.frame(cox_mat, net1=pro_act1, net2=pro_act2)
        cox_res <- coxph(Surv(D_PFS, D_PFS_FLAG) ~ net1 + net2, data=cox_tmp)  
        cox_pval <- summary(cox_res)$logtest[3]
        
        if(cox_pval <= pval_cut) {
          tree <- c(pro1, pro2, pro_net1, pro_net2, paste(pro1, pro2, sep="_"), summary(cox_res)$logtest[3], 
                    low_perc, high_perc)
          outie[[paste(pro1, pro2, sep="_")]] <- tree
        }
      }
    } ## end for pro2
  } ## end for pro1
  
  ## no results so return NULL
  if(length(outie) <= 2) {
    return(NULL)
  }
  ret_all <- data.frame(do.call(rbind,outie))
  colnames(ret_all) <- c("pro1", "pro2", "pro1_act", "pro2_act", "both", "cox_pval", "low_perc", "high_perc")
  
  ret_all %>% 
    dplyr::arrange(desc(high_perc), low_perc, cox_pval) %>%
    dplyr::mutate(pval_rank=order(cox_pval)) %>%
    dplyr::mutate(low_perc_rank=order(low_perc)) ->
    ret_all
  
  return(ret_all)
} ## end getRanking



## prune networks by calculating risk ratios for active/inactive programs. For use in sapply function 
## order of x must match order of risk$sample
## x - row of data.frame of network activity for a regulon
## risk - table of risk meta_data with column labeles risk (low, high)
## min - minimum number of subjects with active activity (-1,1)
## perc_min - max percentage (or 1- max percentage) for significance
getRiskRatio <- function(x, risky=mm_risk$risk, min=10, perc_min=0.3) {
  
  down <- up <- FALSE
  tab <- table(x, risky)
  
  downer <- rownames(tab) %in% "-1"         ## get row of inactive counts if it exists
  if(sum(downer) > 0) {                    
    tmp <- tab[downer,]
    if(sum(tmp) > min) {                     ## check if subject count is greater than min
       rat <- tmp[1]/sum(tmp)
       if((rat <= perc_min) | (rat >= (1-perc_min))) {
         down <- TRUE
       }
    }
  }
  
  upper <- rownames(tab) %in% "1"         ## get row of inactive counts if it exists
  if(sum(upper) > 0) {                    
    tmp <- tab[upper,]
    if(sum(tmp) > min) {                     ## check if subject count is greater than min
      rat <- signif(tmp[1]/sum(tmp), 3)
      if((rat <= perc_min) | (rat >= (1-perc_min))) {
        up <- TRUE
      }
    }
  }
  
  return(c(down, up))
}

## prune networks by t-test for all subjects. For use in sapply function 
## order of x must match order of risk$sample
## x - row of data.frame of network activity for a regulon
## risk - table of risk meta_data with column labeles risk (low, high)
myT <- function(x, risk=mm_risk$risk) {
  risky <- x[risk=="high"]
  no_risky <- x[risk=="low"]
  if((var(risky)==0) & (var(no_risky)==0)) {
    return(1)
  } else {
    return(t.test(risky, no_risky)$p.value)
  }
}

## chi square test on risk
calcRisk <- function(x, risky=classer) {
  tab <- table(x, risky)
  return(summary(tab)$p.value)
}

## prune networks by t-test for all subjects. For use in sapply function 
## order of x must match order of risk$sample
## x - row of data.frame of network activity for a regulon
## risk - table of risk meta_data with column labeles risk (low, high)
my_LM <- function(x, var=mm_risk$GuanScore) {
  
  if(var(x) == 0) {
    return(1)
  } else {
     res <- summary(lm(var~x))
     return(res$coefficients[2,4])
  }
}

## get table of risk for a regulon/program
## regg - identifier of regulon/program
## mat - network activity matrix or vector of network activity for that
##       particular program
## risky - vector of risk
getTableSimple <- function(regg, mat=dat, risky=classer) {
  
    if(is.null(dim(mat))) {     ## vector
       nett <- mat
    } else {               ## matrix/data.frame
       nett <- unlist(mat[regg,])
    }
  
    tab <- as.data.frame.matrix(table(nett, risky))
    if(!("extreme" %in% colnames(tab))) {
      tab <- cbind(extreme=rep(0, nrow(tab)), tab)
    }
    
    tab <- cbind(total=apply(tab, 1, sum), tab)
    tab <- rbind(total=apply(tab, 2, sum), tab)
    percs <- round((tab$low/(tab$extreme + tab$high + tab$low))*100)
    tab$perc_low <- percs
    return(tab)
  }

## get stats for top node of decision tree
## risky - phenotype data frame 
## subb - name of subtype
getTopNode <- function(risky, subb) {
  
  risk_fac <- factor(risky$extreme_risk, levels=c("extreme", "high", "low"))
  prog_samp <- rep(NA, 3)
  risk_samp <- unlist(table(risk_fac))
  prog_pfs <- rep(NA, 3)
  prog_pfs_down_inactive <- NA
  risk_pfs <- tapply(risky$D_PFS, risk_fac, median)
  total_pfs <- median(risky$D_PFS)
  
  out_df <- matrix(c(prog_samp, prog_pfs, prog_pfs_down_inactive, risk_samp, 
                     risk_pfs, total_pfs, NA), nrow=1)
  out_vec <- data.frame(subb, NA, out_df)
  colnames(out_vec) <- c("feature", "risk_net_value",
                         "active_down_samples", "inactive_samples", "active_up_samples",
                         "active_down_med_pfs_days", "inactive_med_pfs_days",
                         "active_up_med_pfs_days", "inactive_down_med_pfs_days",
                         "extreme_risk_samples", "high_risk_samples", "low_risk_samples",
                         "extreme_risk_med_pfs_days", "high_risk_med_pfs_days",
                         "low_risk_med_pfs_days", "total_pfs", "chi_pval")
  return(out_vec)
}

## get sample and median pfs statistics for subjects in a node
## prog - name of program
## net_val - high risk network value (-1, 1, 1) 
## prog_net - vector of network activity for prog
## risky - data.frame of pheno information for subjects in this node
getProStats <- function(prog, net_val, prog_net, risky=risk_sub) {
  
  net_fac <- factor(prog_net, levels=c(-1, 0, 1))
  net_dual <- prog_net
  net_dual[net_dual==-1] <- 0
  
  risk_fac <- factor(risky$extreme_risk, levels=c("extreme", "high", "low"))
  prog_samp <- unlist(table(net_fac))
  risk_samp <- unlist(table(risk_fac))
  prog_pfs <- tapply(risky$D_PFS, net_fac, median)
  prog_pfs_down_inactive <- median(risky$D_PFS[prog!=1]) 
  risk_pfs <- tapply(risky$D_PFS, risk_fac, median)
  total_pfs <- median(risky$D_PFS)
  
  out_df <- matrix(c(prog_samp, prog_pfs, prog_pfs_down_inactive, 
                     risk_samp, risk_pfs, total_pfs), nrow=1)
  out_vec <- data.frame(prog, net_val, out_df)
  colnames(out_vec) <- c("feature", "risk_net_value",
                      "active_down_samples", "inactive_samples", "active_up_samples",
                      "active_down_med_pfs_days", "inactive_med_pfs_days",
                      "active_up_med_pfs_days", "inactive_down_med_pfs_days",
                      "extreme_risk_samples", "high_risk_samples", "low_risk_samples",
                      "extreme_risk_med_pfs_days", "high_risk_med_pfs_days",
                      "low_risk_med_pfs_days", "total_pfs")
  return(out_vec)
} ## end getProStats


## Creae decision tree for a specific subtype and export as pdf
## tree- table of tree info
## pfs - median pfs survival times from Surv object
## leaf_info - data.frame showing risk percentages for important nodes
## output_path - path to save file
generateDecisionTreePdf_model <- function(tree, pfs, leaf_info, out_path=out_path) {

    ## read in tree info produced by decision_tree_assembly_'subtype'.Rmd
    ##tree <- as.data.frame(read_csv(here("output/decision_tree/amp1q_tree.csv")))
  ## get subtype
  subtype <- tree[1,1]
  sub_lab <- paste(tree[2,1], tree[4,1] , sep="_")
  
  ##make graph structure
  g1 <- graph(edges=c(1,2, 1,3, 2,4, 2,5))
  
  ## generate pie distributions
  pie_mat <- as.matrix(tree[,10:12])
  total_subs <- apply(pie_mat, 1, sum)
  percs <- signif((total_subs/total_subs[1])*100, 3)
  pie_colors <- list(c("red4", "red3", "blue"))
  V(g1)$pie.color <- pie_colors      
  node_pie <- list(pie_mat[1,], pie_mat[2,], pie_mat[3,], pie_mat[4,], pie_mat[5,])
  
  ## generate node labels (note this assumes high risk is always net activity==1)
  node1_lab <- paste(subtype, "\n", 
                     "n=", total_subs[1])
  
  node2_name <- paste0("Pr-", tree[2,1], ifelse(tree[2,2] == 1, "(+)", "(<=0)")) ## top right branch
  node2_lab <- paste0(node2_name, "\nn=", total_subs[2])
  
  ## leaf 1
  node3_name <- paste0("Pr-", tree[3,1], ifelse(tree[3,2] == 1, "(+)", "(<=)"))
  node3_lab <-  paste0(node3_name, "\nn=", 
                       total_subs[3], "\n\n\n\n\nLow SYGNAL risk (", percs[3], "%)\nmed PFS: ", 
                       pfs[1], " m")
  
  ## leaf 3
  node4_name <- paste0(node2_name,  paste0("  Pr-", tree[4,1], ifelse(tree[4,2] == 1, "(+)", "(-)")))
  node4_lab <-  paste0("  Pr-", tree[4,1], ifelse(tree[4,2] == 1, "(+)", "(-)"), 
                       "\nn=", total_subs[4], "\n\n\n\n\nExtreme SYGNAL risk (", percs[4], "%)\nmed PFS: ", 
                       pfs[3], " m")
  
  ## leaf 2
  leaf2_sign <- ifelse(tree[4,2] == 1, "(<=0)", "(>=0)")
  node5_lab <-  paste0("Pr-", tree[5,1], leaf2_sign, 
                       "\nn=", total_subs[5], "\n\n\n\n\nHigh SYGNAL risk (", percs[5], "%)\nmed PFS: ",
                       pfs[2], " m")
  
  node_labs <- c(node1_lab, node2_lab,node3_lab,node4_lab,node5_lab)
  
  ## generate edge labels (p-values). spaces are hack to position label
  edge_lab <- c(paste("\np-value:", signif(tree$chi_pval[2], 3), "                         "), 
                "",
                paste("\np-value:", signif(tree$chi_pval[4], 3), "                         "),
                "")
  
  ## adjust node labels
  vert_loc <- c(-pi/2,-pi/2, pi/2, pi/2, pi/2.1)
  vert_dist <- c(6,6,0, 0, 0)
  
  ## leaf vertex border colors
  vert_label_col <- c("black", "black", "burlywood", "darkorange4", "darkorange")
  
  ## this lays tree out on a left node traversal. Multiply the first column by -1
  ## to convert to right node traversal
  l1 <- layout_as_tree(g1, root=1, mode="out")
  l1[,1] <- l1[,1]*-1
  
  ## calculate important node risk percentages
  cols <- c(" ", "node", "risk", "%")
  percs <- paste0(leaf_info$percs, "%")
  nodes <- c(node3_name, 
             node2_name,
             node2_name, 
             node4_name)
  risk <- c("low risk", "high risk", "extreme risk", "extreme risk")
  risk1 <- c("low", "high", "extreme", "extreme")
  leafs <- data.frame(node=nodes, risk=risk, percs=percs)
  
  pie_legend <-c(paste0("extreme n=", pie_mat[1,1]), 
                 paste0("high n=", pie_mat[1,2]), 
                 paste0("low n=", pie_mat[1,3]))
                          
  ## print plot to markdown
  marr <- c(0,1,0,3)
  par(mar=marr)
  plot(g1, 
       layout=l1, 
       vertex.size=50,
       vertex.label=node_labs,         ## node labels
       vertex.label.dist=vert_dist,    ## set label distance from node
       vertex.label.degree=vert_loc,   ## set label location
       vertex.label.cex=1.1,           ## set node label size
       vertex.label.font=1.3,
       vertex.label.color=vert_label_col,
       vertex.shape="pie",             ## make node pie chart
       vertex.pie=node_pie,            ## values for node pie charts
       edge.color="black",
       edge.label=edge_lab,            ## add p-values to select edges
       edge.label.cex=1,              ## set edge labe size  
       edge.label.font=2,
       edge.label.color="black",
       rescale=TRUE, asp=0.60
       ##  main=tree[1,1]
  )
  legend("topleft", bty="n", title="SYGNAL risk",
         legend=pie_legend, 
         fill=unlist(pie_colors))
  
  addtable2plot("topright", table=leafs, display.colnames=FALSE, title="risk")
  
  ## plot the graph
  pdf(file=paste0(out_path, subtype, "_", sub_lab ,"_decision_tree.pdf"), height=5)
  par(mar=marr)
  plot(g1, 
       layout=l1, 
       vertex.size=50,
       vertex.label=node_labs,         ## node labels
       vertex.label.dist=vert_dist,    ## set label distance from node
       vertex.label.degree=vert_loc,   ## set label location
       vertex.label.cex=1.1,           ## set node label size
       vertex.label.font=1.3,
       vertex.label.color=vert_label_col,
       vertex.shape="pie",             ## make node pie chart
       vertex.pie=node_pie,            ## values for node pie charts
       edge.color="black",
       edge.label=edge_lab,            ## add p-values to select edges
       edge.label.cex=1,              ## set edge labe size  
       edge.label.font=2,
       edge.label.color="black",
       rescale=TRUE, asp=0.60
       ##  main=tree[1,1]
  )
  legend("topleft", bty="n", title="survival",
         legend=pie_legend, 
         fill=unlist(pie_colors))
  addtable2plot("topright", table=leafs, display.colnames=FALSE, title="risk")
  
  dev.off()
} ## end generateDecisionTreePdf

## make decision tree survival plot by node. Saves survival object for n=1 script
## pr1 - first node program
## pr2 - second node program
## act1 - activity of first program (-1,1)
## act2 - activity of second program (-1,1),
## subtype - label for subtype (translocation, mutation, ...)
## progy - program activity
## risky - risk pheno data
## save_plot - if TRUE save plot
makeSurvivalPlot <- function(pr1, pr2, act1, act2, progy=prog_sub, risky=risk_sub, subtype,
                             save_plot=TRUE, outpath=out_path) {
  
  ## get network activity for the two programs
  pro1_net <- unlist(progy[pr1,])
  pro2_net <- unlist(progy[pr2,])
  
  l2_act <- ifelse(act2==1, "(<=0) n=", "(>=0) n=")
  l3_act <- ifelse(act2==1, "(+) n=", "(-) n=")
  
  ## get sample sizes of leaves
  sub_samp <- apply(out_stats[3:5,10:12], 1, sum)
  
  ## create index for the three leaves of decision tree
  leaf1 <- paste0("Pr-", pr1,  "(<=0) n=", sub_samp[1])
  leaf2 <- paste0("Pr-", pr1, "(+) & Pr-", pr2, l2_act, sub_samp[2])
  leaf3 <- paste0("Pr-", pr1, "(+) & Pr-", pr2, l3_act, sub_samp[3])
  surv_ind <- rep(leaf1, length(pro1_net))
  surv_ind[(pro1_net==act1) & (pro2_net !=act2)] <- leaf2
  surv_ind[(pro1_net==act1) & (pro2_net==act2)] <-leaf3
  surv_ind <- factor(surv_ind, levels=c(leaf1, leaf2, leaf3), ordered=TRUE)
  leaf <- surv_ind
  
  ## get survival data
  surv2 <-  survfit(Surv(time = PFS, event=D_PFS_FLAG)~leaf, data=risk_sub)
  ## ggsurvplot(surv2)
  
  labs <- names(surv2$strata)
  labs <- str_remove(labs, "leaf=")
  
  ## get med PFS (50% survival)
  med_pfs <- summary(surv2)$table[,7]
  names(med_pfs) <- labs
  pfs_tab <- data.frame(pfs=med_pfs, y1=rep(0.5, 3), y2=rep(0,3), activity=names(med_pfs))
  
  ## generate plot
  labs <- names(surv2$strata)
  labs <- str_remove(labs, "leaf=")
  tab <- surv2$strata
  plot_ind <- factor(c(rep(labs[1], tab[1]), rep(labs[2], tab[2]), rep(labs[3], tab[3])),
                     levels=labs, ordered=TRUE)
  surv_df <- data.frame(activity=plot_ind, PFS=surv2$surv, time=surv2$time)
  
  ## calculate 50% survival probability

  g_surv <- ggplot(surv_df, aes(x=time, y=PFS, color=activity)) + geom_point() + geom_line() +
    scale_color_manual(values=c("burlywood", "darkorange", "darkorange4")) +
    theme(legend.justification=c(1,1), legend.position=c(1,1),
          legend.background=element_rect(fill="transparent")) + 
    xlab("actual clinical outcome (PFS months)") + ylab("survival probability") +
    ggtitle(paste("KM survival")) + 
    geom_hline(yintercept = 0.5, linetype="dotted") +
    geom_segment(data=pfs_tab, mapping=aes(x=pfs, xend=pfs, y=y1, yend=y2),
                 linetype="dashed") +
    theme(text = element_text(size = 15)) +
    theme(aspect.ratio=4/4)
  print(g_surv)
  
  if(save_plot) {
      combo <- paste(pr1, pr2, sep="_")
      ggsave(paste0(outpath, subtype, "_",  combo, "_survival_plot.pdf"), device="pdf")
  }
  return(list(surv2, surv_df))
}


## generate decision tree info for one program
## pro- label of program/regulon
## mat - network activity for subset of subjects belonging
##       to this node
## pheno - phenotype data for subset of subjects belonging
##         to this node.
## regs - regulon network activity
## prog_reg - vector of regulons for this program
## risk_net_value - network value (-1,0,1) that defines high-risk group
decisionTreeProgramInfo <- function(pro, mat, pheno, 
                                    pro_reg=unlist(prog_regs[pro]),
                                    reg_net=dat, risk_net_value=1) {
  
    pro_net <- unlist(mat[pro,])
    print(paste("program", pro, ":", length(pro_net), "subjects"))
    
    ## get risk distribution and median pfs for each network value
    print("all network values")
    pro_pfs <- signif(tapply(pheno$D_PFS, 
                             pro_net, median)/30, 4)
    pro_pfs <-c(signif(median(pheno$D_PFS)/30, 3), pro_pfs)
    tab <- getTableSimple(pro, mat=mat, risky=pheno$extreme_risk)
    tab$med_pfs_mo <- pro_pfs
    print(kable(tab, format="simple", caption="risk distribution"))
    vioplot(pfs~group, data.frame(pfs=pheno$D_PFS, group=pro_net), main="PFS") 
    
    ## get risk distribution and median pfs for combined inactive and down active
    print("Combined inactive and down active network values")
    pro_net_min <- ifelse(pro_net==1, 1, 0)
    pro_pfs_min <- signif(tapply(pheno$D_PFS, 
                             pro_net_min, median)/30, 4)
    pro_pfs_min <-c(signif(median(pheno$D_PFS)/30, 3), pro_pfs_min)
    tab_min <- getTableSimple(pro, mat=pro_net_min, risky=pheno$extreme_risk)
    tab_min$med_pfs_mo <- pro_pfs_min
    print(kable(tab_min, format="simple", caption="risk distribution"))
    vioplot(pfs~group, data.frame(pfs=pheno$D_PFS, group=pro_net_min), main="PFS") 
    
    
    ## calculate percentages of risk for this node
    percs <- signif((tab[,2:4]/tab$total)*100, 3)
    print(kable(percs, format="simple", caption="risk %"))
   
    ### survival plot by network activity
    pheno$network <- pro_net
    colors <- c("blue", "green", "red")
    surv1 <-  survfit(Surv(time = D_PFS, event=D_PFS_FLAG)~network, 
                      data=pheno)
    g1 <- ggsurvplot(surv1, data=pheno, palette=colors) + 
      ggtitle("network activity") + 
      xlab("survival: days")
    
    colors <- c("red", "green", "blue")
    pheno1 <- pheno
    pheno1$risk <- pheno1$extreme_risk
    surv2 <-  survfit(Surv(time = D_PFS, event=D_PFS_FLAG)~risk, 
                      data=pheno1)
    g2 <- ggsurvplot(surv2, data=pheno1, palette=colors) + 
      ggtitle("risk") + 
      xlab("survival: days")
  
    print(g1)
    print(g2)
    
    ## heatmap of regulon activity for this program
    
    ## Order by risk (low, high, extreme)
    pheno %>%
      dplyr::arrange(GuanScore) ->
      pheno_rev
    reggie <- reg_net[pro_reg,pheno_rev$sample]
    if(sum(colnames(reggie) != pheno_rev$sample) >0) stop("program mismatch")
    
    ## plot extreme/high/low risk
    sub_lab <- factor(pheno$extreme_risk, levels=c("low", "high", "extreme"),
                      ordered=TRUE)
    sub_color <- factor(sub_lab,
                        levels=unique(sub_lab),
                        labels=colors)
    names(sub_color) <- sub_lab
    pfs <- HeatmapAnnotation(subtype=sub_lab, col=list(subtype=sub_color) )
    hm_bin <- Heatmap(as.matrix(reggie), 
                      column_title="Network activity by risk", 
                      row_title="programs",
                      cluster_rows=TRUE, 
                      cluster_columns=TRUE,
                      show_column_names=FALSE, 
                      top_annotation=pfs, 
                      column_split=paste(sub_lab, sep="_"),
                      row_gap=unit(0.2, "mm"),
                      row_names_gp=gpar(fontsize=10),
                      heatmap_legend_param=list(direction="horizontal",
                                                title="network activity") ) 
    draw(hm_bin, heatmap_legend_side="top", annotation_legend_side="right") 
    
    ## show high/low risk
    sub_lab <- factor(pheno$risk, levels=c("low", "high"),
                      ordered=TRUE)
    sub_color <- factor(sub_lab,
                        levels=unique(sub_lab),
                        labels=c("red", "blue"))
    names(sub_color) <- sub_lab
    pfs <- HeatmapAnnotation(subtype=sub_lab, col=list(subtype=sub_color) )
    hm_bin <- Heatmap(as.matrix(reggie), 
                      column_title="Network activity by risk", 
                      row_title="programs",
                      cluster_rows=TRUE, 
                      cluster_columns=TRUE,
                      show_column_names=FALSE, 
                      top_annotation=pfs, 
                      column_split=paste(sub_lab, sep="_"),
                      row_gap=unit(0.2, "mm"),
                      row_names_gp=gpar(fontsize=10),
                      heatmap_legend_param=list(direction="horizontal",
                                                title="network activity") ) 
    draw(hm_bin, heatmap_legend_side="top", annotation_legend_side="right") 
    
    
    ## order by Guan score
    hm_bin <- Heatmap(as.matrix(reggie), 
                      column_title="Network activity by Guan score", 
                      row_title="programs",
                      cluster_rows=TRUE, 
                      cluster_columns=FALSE,
                      show_column_names=FALSE, 
                      top_annotation=HeatmapAnnotation(guan=pheno_rev$GuanScore), 
                      row_gap=unit(0.2, "mm"),
                      row_names_gp=gpar(fontsize=10),
                      heatmap_legend_param=list(direction="horizontal",
                                                title="network activity") ) 
    draw(hm_bin, heatmap_legend_side="top", annotation_legend_side="right") 
    
    ## plot D_PFS
    ## Order by D_PFS
    pheno %>%
      dplyr::arrange(-D_PFS) ->
      pheno_pfs
    reggie <- reg_net[pro_reg, pheno_pfs$sample]
    event <- ifelse(pheno_pfs$D_PFS_FLAG==1, "yes", "no")
    sub_lab <- factor(event, levels=c("yes", "no"))
    sub_color <- factor(sub_lab,
                        levels=unique(sub_lab),
                        labels=c("red", "blue"))
    names(sub_color) <- sub_lab
    pfs <- HeatmapAnnotation(subtype=sub_lab, col=list(subtype=sub_color) )
    hm_bin <- Heatmap(as.matrix(reggie), 
                      column_title="Network activity by D_PFS", 
                      row_title="programs",
                      cluster_rows=TRUE, 
                      cluster_columns=FALSE,
                      show_column_names=FALSE, 
                      top_annotation=HeatmapAnnotation(pfs=pheno_pfs$D_PFS, event=sub_lab), 
                      row_gap=unit(0.2, "mm"),
                      row_names_gp=gpar(fontsize=10),
                      heatmap_legend_param=list(direction="horizontal",
                                                title="network activity") ) 
    draw(hm_bin, heatmap_legend_side="top", annotation_legend_side="right") 
    
    pro_ind <- pro_net == risk_net_value
    out_high <- getProStats(pro, risk_net_value, pro_net[pro_ind], risky=pheno[pro_ind,] )
    out_low <- getProStats(pro, paste("not", risk_net_value, sep="_"), pro_net[!pro_ind], risky=pheno[!pro_ind,] )
   
    return(out_stats)
} ## end decisionTreeProgramInfo

## Apply elastic net regression
## libraries: caret, pROC, ggplot2
## data - data.frame of network activity, rows are features, columns are samples
## classer - vector of risk labels (high, low). must match column order of data
## ml_method - name of caret ml method
## tuner - number of combinations of tuning parameter
## seed - random seed
## prune - if true remove low info features
## prune_cut - threshold for pruning (minimum number of absolute active features necessary)
## boot_iters - number of bootstrapping iterations
applyElasticNet <- function(data, classer, ml_method="glmnet", tuner=20, seed=123,
                                 prune=TRUE, prune_cut=3, boot_iters=10) {
  
  pro <- data.frame(t(as.matrix(data)))
  colnames(pro) <- as.character(rownames(data))
  input <- pro
  
  classer <- ifelse(classer=="low", "low", "high")
  
  ## prune low info programs
  if(prune) {
     act <- apply(pro, 2, function(x) return(sum(abs(x))))
     input <- pro[,act >prune_cut]
  }
  
  print(paste("Model consists of", ncol(input), "programs"))
  input <- cbind(risk=classer, input)
  
  set.seed(seed)
  model <- caret::train(risk~., data=input, method = ml_method,
    trControl = trainControl("boot", number = boot_iters),
    tuneLength = tuner)
  
  ## get predictions
  preds <- predict(model, input[,-1], type="prob")
  out_preds <- data.frame(preds=preds[,1], risk=classer, sample=rownames(input))
  
  ## get predicted groups
  pred_group <- rep("low", nrow(preds))
  pred_group[preds[,1] > 0.5] <- "high"
  pred_group[preds[,1] > 0.6] <- "extreme"
  
  
  rocc <- roc(response=classer, predictor=preds[,1], plot=FALSE)
  auc1 <-  signif(auc(rocc)[1], 3)
  roc_d1 <- data.frame(FPR=rev(1-rocc$specificities), TPR=rev(rocc$sensitivities))
  
  ggplot(roc_d1,aes(FPR,TPR))+geom_line(linewidth = 2, alpha = 0.7) +
    labs(title= "Validation data ROC", 
         x = "False Positive Rate", 
         y = "True Positive Rate") +
    annotate('text', x=0.05, y=1, label="AUC", color="black", fontface = 'bold') +
    annotate('text', x=0.05, y=0.95, label=auc1, color="black", fontface = 'bold')  +
    geom_abline(slope=1, intercept=0, linetype="dashed")
  
  return(list(model, rocc, out_preds, pred_group))
}


## dependencies library(survminer)
## net_train - network data for training set (mm model)
## net_test - network data for test data (validation)
## train_class - classes for train data
## pheno_test - phenotype data for test data
## ml_method - ranger or glmnet (ridge tune=10)
## tuner - tuning parameter for ridge regression (set to 10)
applyRiskValidation <- function(net_train, net_test, train_class, pheno_test, ml_method="ranger",
                                tuner=10) {
  
  ## create generic trainControl object for machine learning training
  tc <- trainControl(method="cv", number=10, p=0.8, classProbs=TRUE,
                     savePredictions=TRUeE)
  
  tuneGridd <- NULL
  if(ml_method=="glmnet") {
     tuneGridd <- expand.grid(alpha=seq(0,1,0.1), lambda=seq(0.0001, 1, length=20))
  }
  
  ## full set of predictors
  s_time1 <- system.time(rf_fit <- train(x=net_train, y=train_class, method=ml_method,
                                         trControl=tc, importance = "permutation", 
                                         tuneLength=tuner, tuneGrid=tuneGridd))
  
  print(paste("Model evaluation with model data: pruned samples=", ncol(net_train)))
  x <- evalm(rf_fit, plots=c("r"))
  
  ## format test data and classifier
  valid <- t(net_test[,-1])
  colnames(valid) <- as.character(rownames(net_test)) 
  valid <- valid[pheno_test$sample,]
  if(sum(rownames(valid) != pheno_test$sample) >0) stop("colnames mismatch")
  pheno_test$risk_lab <- plyr::mapvalues(pheno_test$risk, from=c(0,1), to=c("low", "high"))
  test_factor <- factor(pheno_test$risk_lab)
  
  preds <- caret::predict.train(rf_fit, valid, type="prob")
  pred1 <- preds[,1]
  names(pred1) <- rownames(valid)
  
  pred_lab <- ifelse(pred1 > 0.5, "high", "low")
  rocc <- roc(response=pheno_test$risk, predictor=pred1, plot=TRUE)
  print(data.frame(predicted=table(pred_lab), truth=table(pheno_test$risk_lab)))
  print(paste("AUC:", signif(auc(rocc)[1], 3)))
  
  pheno_test$predicted <- pred1
  pheno_test$predict_lab <- pred_lab
  pheno_test %>%
    dplyr::arrange(predicted) %>%
    ggplot(aes(x=1:nrow(pheno_test), y=predicted, color=risk_lab)) + geom_point() +
    ggtitle("predicted risk")->
    g1
  
  g2 <- ggplot(pheno_test, aes(x=risk_lab, y=predicted, color=risk_lab)) + geom_boxplot() + 
    ggtitle("predicted risk")
  g3 <- ggplot(pheno_test, aes(x=predict_lab, y=D_PFS, color=predict_lab)) + 
    geom_boxplot() + ggtitle("predicted risk")
  grid.arrange(g1,g2,g3, ncol=2)
  
## KM plot
surv_predict <- survfit(Surv(time = D_PFS, event=D_PFS_FLAG)~predict_lab, 
                 data=pheno_test)
surv_truth <- survfit(Surv(time = D_PFS, event=D_PFS_FLAG)~risk_lab, 
                  data=pheno_test)
surv_both <-  survfit(Surv(time = D_PFS, event=D_PFS_FLAG)~predict_lab +risk_lab, 
                      data=pheno_test)
##ggsurvplot(surv_predict, data=pheno_test) + ggtitle("Predicted risk")
    
if(!is.null(surv_predict$strata) & !is.null(surv_truth$strata)) {
    df_predict <- data.frame(method=rep("predicted", length(surv_predict$time)), 
                     groups= c(rep("high_risk",  surv_predict$strata["predict_lab=high"]),
                               rep("low_risk",  surv_predict$strata["predict_lab=low"])),
                     days=surv_predict$time, 
                     risk=surv_predict$surv)
    df_truth <- data.frame(method=rep("observed", length(surv_truth$time)), 
                             groups= c(rep("high_risk",  surv_truth$strata["risk_lab=high"]),
                                       rep("low_risk",  surv_truth$strata["risk_lab=low"])),
                             days=surv_truth$time, 
                             risk=surv_truth$surv)
    df_all <- rbind(df_predict, df_truth)
    ggplot(df_all, aes(x=days, y=risk, color=method)) + 
      geom_line(aes(linetype=groups))
  } else {
    print("Survival plots not possible")
  }
  
  ## get individual AUCs for the ROC for each variable
  print(paste("Training data: Model evaluation by feature"))
  roc_train <- filterVarImp(x=as.data.frame(net_train), y=train_class, nonPara=TRUE)
  roc_train$regulon <- colnames(net_train)
  roc_train %<>% dplyr::arrange(-high)
  boxplot(roc_train[,2], main="AUC: individual features")
  
  print(paste("test data: Model evaluation by feature"))
  roc_test <- filterVarImp(x=as.data.frame(valid), y=test_factor, nonPara=TRUE)
  roc_test$regulon <- colnames(valid)
  roc_test %<>% dplyr::arrange(-high)
  boxplot(roc_test[,2], main="AUC: individual feature")
  
  roc_all <- data.frame(train_feature=roc_train[,3], train_auc=signif(roc_train[,2],3), 
                   test_feature=roc_test[,3], test_auc=signif(roc_test[,2],3))
  rownames(roc_all) <- NULL
  
  return(list(train=roc_train, test=roc_test, features=roc_all))
}


## choose prediction function based on what is being predicted. See riskSub or guanSub
## for paramter definitions
## class - risk - riskSub, others - GuanSub
chooseSub <- function(reg, subb, class="GuanScore", risk=mm_risk, ml_method="ranger", cut=0.01,
               tc=generic_tc, num_feats=150, glm_meth=NULL) {
  if(class=="risk") {
     res <- riskSub(reg, subb, class, risk, ml_method, cut, tc, num_feats, glm_meth) 
  }else {
     res <- guanSub(reg, subb, class, risk, ml_method, cut, tc, num_feats, glm_meth) 
  }
  return(res)
}


## Apply ML methodsd for guanscore or d_pfs prediction
## reg - network activity for regulons, row=regulons, column=samples
## subb - subtype (or all for all subjects)
## class - class to be  predicted, must be columns in risk table 
##         (GuanScore, D_PFS)
## risk - table of risk meta_data with column labeles GuanScore or D_PFS
## ml_method: ranger or glmnet (set glm_meth)
## glm_meth: either ridge or elastic_net
## cut: fdr cutoff for initial pruning
## tc: training control object for caret train method
## num_feats: maximum number of features 
guanSub <- function(reg, subb, class="GuanScore", risk=mm_risk, ml_method="ranger", cut=0.01,
                    tc=generic_tc, num_feats=150, glm_meth=NULL) {
  
  ## set  up parameters for regression methods
  tune = 1          ## for ridge
  tuneMat = NULL    ## for elastic_net
  if(ml_method=="glmnet") {
    if(glm_meth=="elastic_net") {
      tuneMat <- expand.grid(alpha=1, lambda = 10^seq(-3, 3, length = 100))
    } else {
      tune=10
    }
  }
  
  ## select samples for this subtype
  risk %>%
    dplyr::filter(!!(as.name(subb)) == 1) ->
    risk_sub
  
  classer <- unlist(risk_sub[,class])
  
  reg_sub <- data.frame(X1=reg$X1, reg[,risk_sub$sample])
  
  pvals <- apply(reg_sub[,-1], 1, my_LM, var=risk_sub$GuanScore)
  fdrs <- p.adjust(pvals, "fdr")
  pval_df <- data.frame(value=c(pvals, fdrs), 
                        type=factor(rep(c("pvalue", "fdr"), each=length(pvals)),
                                    levels=c("pvalue", "fdr"), ordered=TRUE))
 g1 <-  ggplot(pval_df, aes(x=value)) + geom_histogram(bins=20) + 
    facet_wrap(~type, scales = "free")
 print(g1)
 
  input <- t(reg_sub[fdrs<cut,-1]) 
  
  ## apply ML method
  s_time1 <- system.time(rf_fit <- train(x=input, y=classer, method=ml_method,
                                         trControl=tc, importance = "permutation", tuneLength=tune,
                                         tuneGrid=tuneMat))
  preds <- predict(rf_fit)
  
  cat("Inital model with ", ncol(input), "regulons  \n  \n")
  
  corrs <- signif(cor(preds, classer), 3)
  ran <- range(c(preds, classer))
  dat_all <- data.frame(predicted=preds, pfs=risk_sub$D_PFS, truth=classer, 
                        relapse=factor(risk_sub$D_PFS_FLAG, levels=c(0,1), labels=c("no", "yes")))
  dat_dt <- melt(data.table(dat_all), id.vars=c("relapse", "pfs"), value.name="Guan")
  
  g2 <- ggplot(dat_all, aes(x=truth, y=predicted, color=relapse)) +
    geom_point() + geom_smooth(method="loess", col="blue")+ ggtitle(paste("cor:", corrs)) +
    ylim(ran) + xlim(ran) + geom_abline(slope=1, intercept=0, linetype="dashed")

  g2a <- ggplot(dat_dt, aes(x=Guan, y=pfs, color=relapse, group=variable)) + 
    geom_point(aes(shape=variable)) +
    scale_shape_manual(values=c(21,2))
    
  grid.arrange(g2, g2a)
  
  ## Get most important features in order
  varImp(rf_fit, scale=TRUE)$importance %>%
    dplyr::arrange(-Overall) ->
    imps
  
  ## get individual AUCs for the ROC for each variable
  ind_lm <- apply(input, 2, function(x,y) signif(cor(x,y),3), y=classer)
  ind_lm <- ind_lm[order(abs(ind_lm), decreasing=TRUE)]
  boxplot(ind_lm, main="Correlation: individual regulons")
  
  ## iterate with increasing number of features (varImp) to find min set
  num_feats <- min(nrow(imps), num_feats)
  feat_mini <- input[,rownames(imps)[1:num_feats]]  
  
  rfe_control <- rfeControl(functions=rfFuncs, method="boot", number=20)
  s_time3 <- system.time(res <- rfe(feat_mini, classer, rfeControl=rfe_control, 
                                    sizes=seq(1, num_feats, by=2)))   
  ##saveRDS(res, here(paste0("/output/validation/mm_predictor_",class ,".Rds")))
  
  len <- (num_feats/2)+1
  rfe_df <- rbind(data.frame(metric=rep("Rsquared",len), index=1:len,
                             value=res$results$Rsquared),
                  data.frame(metric=rep("RMSE", len), index=1:len,
                             value=res$results$RMSE))
  maxi <- c(which.max(res$results$RMSE), which.max(res$results$Rsquared))
  names(maxi) <- c("RMSE", "Rsquared")
  
  g3 <- ggplot(rfe_df, aes(x=index, y=value)) + geom_point() + 
    facet_wrap(~metric, scale="free") 
  ## + geom_vline(aes(xintercept=maxi), linetype="dashed")
  print(g3)
  
  ## redo model with reduced feature set
  reg_sub %>%
    dplyr::filter(X1 %in% res$optVariables) ->
    tmp
  tmp %>%
    dplyr::select(-X1) %>%
    t() ->
    input
  
  colnames(input) <- tmp$X1
  
  s_time4 <- system.time(rf_fit1 <- train(x=input, y=classer, method=ml_method,
                                          trControl=tc, importance = "permutation", tuneLength=tune,
                                          tuneGrid=tuneMat))
  preds1 <- predict(rf_fit1)
  
  cat("Pruned model with ", ncol(input), "regulons  \n  \n")
  
  corr <- signif(cor(preds1, classer), 3)
  corrs <- c(corrs, corr)
  dat_all$predicted <- preds1
  dat_dt <- melt(data.table(dat_all), id.vars=c("relapse", "pfs"), value.name="Guan")
  g5 <- ggplot(dat_all, aes(x=truth, y=predicted, color=relapse)) +
    geom_point() + geom_smooth(method="loess", col="blue")+ ggtitle(paste("cor:", corrs)) +
    ylim(ran) + xlim(ran) + geom_abline(slope=1, intercept=0, linetype="dashed")
  
  g5a <- ggplot(dat_dt, aes(x=Guan, y=pfs, color=relapse, group=variable)) + 
    geom_point(aes(shape=variable)) +
    scale_shape_manual(values=c(21,2))
  
  grid.arrange(g5, g5a)
  
  ind_out <- data.frame(regulons=names(ind_lm), cor=ind_lm)
  return(list(auc=corrs, opt=res, model=rf_fit1, reg_auc=ind_out))
}



## Apply ML methodsd for risk prediction
## reg - network activity for regulons, row=regulons, column=samples
## subb - subtype (or all for all subjects)
## class - class to be  predicted, must be columns in risk table 
##         (risk, GuanScore, D_PFS)
## risk - table of risk meta_data with column labeles risk (low, high)
## ml_method: ranger or glmnet (set glm_meth)
## glm_meth: either ridge or elastic_net
## cut: fdr cutoff for initial pruning
## tc: training control object for caret train method
## num_feats: maximum number of features 
riskSub <- function(reg, subb, class="risk", risk=mm_risk, ml_method="ranger", cut=0.01,
                    tc=generic_tc, num_feats=150, glm_meth=NULL) {
  
  ## set  up parameters for regression methods
  tune = 1          ## for ridge
  tuneMat = NULL    ## for elastic_net
  if(ml_method=="glmnet") {
    if(glm_meth=="elastic_net") {
      tuneMat <- expand.grid(alpha=1, lambda = 10^seq(-3, 3, length = 100))
    } else {
      tune=10
    }
  }
  
  ## select samples for this subtype
  risk %>%
    dplyr::filter(!!(as.name(subb)) == 1) ->
    risk_sub
  
  classer <- unlist(risk_sub[,class])
  
  reg_sub <- data.frame(X1=reg$X1, reg[,risk_sub$sample])
  
  ## apply t-test to prune regulons
  pvals <- apply(reg_sub[,-1], 1, myT, risk=risk_sub$risk)
  fdrs <- p.adjust(pvals, "fdr")
  pval_df <- data.frame(value=c(pvals, fdrs), 
                        type=factor(rep(c("pvalue", "fdr"), each=length(pvals)),
                                    levels=c("pvalue", "fdr"), ordered=TRUE))
  ggplot(pval_df, aes(x=value)) + geom_histogram(bins=20) + 
    facet_wrap(~type, scales = "free")
  
  input <- t(reg_sub[fdrs<cut,-1]) 
  
  ## apply ML method
  s_time1 <- system.time(rf_fit <- train(x=input, y=classer, method=ml_method,
                                         trControl=tc, importance = "permutation", tuneLength=tune,
                                         tuneGrid=tuneMat))
  preds <- predict(rf_fit, input)
  pfs_means1 <- data.frame(risk=c("low", "high"), 
                           d_pfs=c(mean(risk_sub$D_PFS[preds=="low"]), 
                                   mean(risk_sub$D_PFS[preds=="high"])))
  
  cat("Inital model with ", ncol(input), "regulons  \n  \n")
  
  ## evaluate fit and produce roc curve
  
  all_auc <- x$optres$`Group 1`["AUC-ROC",]
  all_auc <- c(all_auc$Score, as.numeric(unlist(str_split(all_auc$CI, "-"))))
  
 ## print(headerKable(pfs_means1))
  
  ## Get most important features in order
  varImp(rf_fit, scale=TRUE)$importance %>%
    dplyr::arrange(-Overall) ->
    imps
  
  ## get individual AUCs for the ROC for each variable
  s_time2 <- system.time(roc_imp <- filterVarImp(x=as.data.frame(input), 
                                                 y=classer, nonPara=TRUE))
  roc_imp$regulon <- colnames(input)
  roc_imp %<>% dplyr::arrange(-high)
  boxplot(roc_imp[,2], main="AUC: individual regulons")
  
  ## iterate with increasing number of features (varImp) to find min set
  num_feats <- min(nrow(imps), num_feats)
  feat_mini <- input[,rownames(imps)[1:num_feats]]  
  
  rfe_control <- rfeControl(functions=rfFuncs, method="boot", number=20)
  s_time3 <- system.time(res <- rfe(feat_mini, classer, rfeControl=rfe_control, 
                                    sizes=seq(1, num_feats, by=2)))   
  ##saveRDS(res, here(paste0("/output/validation/mm_predictor_",class ,".Rds")))
  
  len <- (num_feats/2)+1
  rfe_df <- rbind(data.frame(metric=rep("Accuracy",len), index=1:len,
                             value=res$results$Accuracy),
                  data.frame(metric=rep("Kappa", len), index=1:len,
                             value=res$results$Kappa))
  maxi <- c(which.max(res$results$Accuracy), which.max(res$results$Kappa))
  names(maxi) <- c("Accuracy", "Kappa")
  
  g1 <- ggplot(rfe_df, aes(x=index, y=value)) + geom_point() + facet_wrap(~metric) 
  ## + geom_vline(aes(xintercept=maxi), linetype="dashed")
  print(g1)
  
  ## redo model with reduced feature set
  reg_sub %>%
    dplyr::filter(X1 %in% res$optVariables) ->
    tmp
  tmp %>%
    dplyr::select(-X1) %>%
    t() ->
    input
  
  colnames(input) <- tmp$X1
  
  s_time4 <- system.time(rf_fit1 <- train(x=input, y=classer, method=ml_method,
                                          trControl=tc, importance = "permutation", tuneLength=tune,
                                          tuneGrid=tuneMat))
  preds1 <- predict(rf_fit1)
  
  cat("Pruned model with ", ncol(input), "regulons  \n  \n")
  
  pfs_means2 <- data.frame(risk=c("low", "high"), 
                           d_pfs=c(mean(risk_sub$D_PFS[preds1=="low"]), 
                                   mean(risk_sub$D_PFS[preds1=="high"])))
  ##print(headerKable(pfs_means2))
  
  ## evaluate fit and produce roc curve
  x <- evalm(rf_fit1, plots=c("r"))
  all_auc1 <- x$optres$`Group 1`["AUC-ROC",]
  all_times <- c(s_time1[3], s_time3[3], s_time4[3])
  aucs= c(all_auc, all_auc1)
  return(list(auc=aucs, opt=res, model=rf_fit1, reg_auc=roc_imp))
}



## calculate deciles of a row of gene expression
## exp - vector of gene expression
## n - number of quantiles 
## returns running mean of lengths ~ length(exp)/n
getPieceWiseMean <- function(exp, n=10) {
  splits <- splitIndices(length(exp), n)
  sapply(splits, function(x, nums=exp) return(mean(nums[x])))
}

## get percentages of subjects with relapse for risk label 'lab'
## lab - risk class (low, extreme, high)
## df data.frame of SYGNAL_class= assigned risk and relapse (yes or no)
## return percentage of subjects with relapse
getRelapse <- function(lab, df) {
  df %>%
    dplyr::filter(SYGNAL_class==lab) ->
    tmp
  out <- round((sum(tmp$relapse=="yes") /nrow(tmp))*100)
  if(is.finite(out)  ==FALSE) {
    out <- NA
  }
  return(out)
}

## get relapse rate at certain year
## lab - risk class (low, extreme, high)
## df data.frame of SYGNAL_class= assigned risk and relapse (yes or no)
## risky - phenotype data for a particular subytype population
## year - what cutoff used to calculate relapse rates  
## return percentage of subjects with relapse
getRelapseByYear <- function(lab, df=prob_df, risky=risk_sub, year=5) {
  
  if(year==5) {  
    months <- max(risky$PFS)  
  } else {  
    months <- year*12  
  }  
  
  df %>%
    dplyr::filter(SYGNAL_class==lab) %>%
    dplyr::left_join(risky[, c("sample", "PFS")], by=c("pid"="sample")) ->
    tmp
  
  
  out <- round((sum(tmp$relapse=="yes" & tmp$PFS <= months) /nrow(tmp))*100)
  if(is.finite(out) ==FALSE) {
    out <- NA
  }
  return(out)
}

## get gene expression for one regulon
## x - regulon id
## reggie - list of regulons where each element is a vector of genes for that
##          regulon
## exp - data.frame of gene expression data, first column is gene id
## gene_network - if TRUE calculate network activity (mean for each subject)
##                else return gene expression
getGE <- function(x, reggie=regulons, exp=as.data.frame(dat), gen_network=FALSE) {
  
  colnames(exp[1]) <- "GENE_ID"
  
  ## select gene expression for this regulon
  exp %>%
    dplyr::filter(GENE_ID %in% reggie[[as.character(x)]]) %>%
    dplyr::mutate(regulon=x) %>%
    dplyr::select(regulon, everything()) ->
    tmp
  
  ## calculate network activity
  if(gen_network) {
    tmp %>% 
      dplyr::select(-regulon, -GENE_ID) %>%
      apply(2, mean) %>%
      t() %>%
      data.frame() %>%
      dplyr::mutate(regulon=x) %>%
      dplyr::select(regulon, everything()) ->
      tmp
  } ## end getGE
  
  return(tmp)
}


## read in translocations and cytogenetic subtypes and collate them
## into a single data.frame
collateSubtypes <- function() {
    ## get MM subtypes  
    tran <- read_csv(paste0(miner_path,
                            "/MINER/data/mutations/translocationsIA12.csv"))[,c("X1", surv$X1)]
    cyto <- read_csv(paste0(miner_path,
                            "/MINER/data/mutations/cytogenetics.csv"))
    
    ## subtypes of interest from Matt's paper
    cyto_sub <- c("del17", "amp1q")
    tran_sub <- c("RNASeq_CCND1_Call", "RNASeq_WHSC1_Call", "RNASeq_MAF_Call",
                  "RNASeq_MYC_Call")
    tran_lab <- c( "CCND1", "MAF", "MYC", "WHSC1")
    cyto_same <- intersect(colnames(cyto[,-1]), surv$X1)
    cyto %>%
      dplyr::filter(X1 %in% cyto_sub) %>%
      dplyr::select(all_of(c("X1", cyto_same))) ->
      cytos
    
    cyto_tab <- table(apply(cytos[,-1], 2, sum))
    
    tran %>%
      dplyr::filter(X1 %in% tran_sub) %>%
      dplyr::mutate(X1=str_split(X1, "_", simplify=TRUE)[,2]) -> ## simplify names
      trans
    
    tran_tab <- table(apply(trans[,-1], 2, sum))
    
    cyto_tmp <-  data.frame(X1=cytos$X1, matrix(0, nrow=2, ncol=ncol(trans)-1))
    colnames(cyto_tmp) <- colnames(trans)
    cyto_tmp[,colnames(cytos)] <- cytos
    
    if((length(setdiff(colnames(cytos[1,]==1), colnames(cyto_tmp[1,]==1))) >0) |
       (length(setdiff(colnames(cytos[2,]==1), colnames(cyto_tmp[2,]==1))) >0) ) {
      stop("cytogenetics not mapped properly")
    }
    
    subs <- rbind(trans, cyto_tmp)
    write_csv(subs, here("output/mm_paper_subtypes.csv" ))

}

###############################################################################
####################### sky92 and gep70 panel coefficients ####################
###############################################################################

## sum weighted gene expression values for sky92 or gep70 genes
## x: matrix of z score gene expression values (TPM normalized). rows are genes
## columns are samples. Only the appropriate genes must be in x for either sky
## or gpe
## sky: Vector of weights for either sky or gpe as defined below.
calcSky <- function(x, sky=bm_entrez) {
  return(t(x)%*%matrix(sky$coefs, ncol=1))
}

## SKY92 https://github.com/bswhite/Celgene-Multiple-Myeloma-Challenge-Baseline-Models/blob/master/publishedClassifiers/emc-92/emc-92.R  
coefficients<-c(0.0594273379801556,-0.110491572253885,-0.108832301005201,-0.062550086484268,-0.0418164824703641,0.0174897923948543,0.00668584689209701,0.0423154071076169,-0.0493203905211073,0.0164925076573208,-0.0344544706411548,0.00866233731636807,0.00455819600852017,-0.0644105573058562,-0.00414107711721288,-0.0183994540236258,-0.0520135932088583,-0.0767532974279582,-0.00063407777663618,0.0489558593887832,-0.0163691988848842,0.0420452994583269,0.0870276088358123,0.0406887458334764,-0.05606504920594,0.0529972408575157,0.0112796922615379,0.0140109440864371,0.00078080860374577,0.075000229909631,0.009265826714752,0.0860546977563988,-0.0996944005531858,0.0558748936987772,0.073011096187736,0.00538232634785429,0.055611094941233,-0.0520445402963303,0.0125985686829356,0.005608194716564,0.0163467078063,-0.0319185908460475,0.0773361612870129,-0.00900009551662273,-0.0575829465263513,0.022119958576764,0.0396248830654306,0.0524561174360331,0.0476703103071683,-0.0422745546301539,-0.0341783399437912,-0.0251675596648738,0.0495596844465701,0.0547508890118559,0.0437224907573062,-0.0371869820707693,0.0205354349665636,0.0445923111233813,-0.0069962048640006,0.025498590737044,0.0208131193381807,0.0685640666167821,-0.0330380997958823,-0.0105781969037762,-0.0585102612125953,0.0128945938676131,-0.0333628118834898,-0.0349253487676344,0.0660971817524709,-0.0617575340318364,-0.0210452256979565,-0.0389953930227488,-0.0873912090180467,-0.0176248165093743,0.0302900524131141,-0.0051520032747514,0.0745893468079515,-0.0322581999457566,0.0200333568025016,0.0115861932788841,-0.00969926642147875,0.00345861530256628,0.0277930303183715,0.0153863421114279,-0.0777964537268526,0.0349480953839344,-0.00022873908620183,-0.0529393125922244,0.0713507977617356,-0.0253569213540041,0.0384170565171902,0.0225490655109955)
names(coefficients)<-c('204379_s_at','202728_s_at','239054_at','202842_s_at','213002_at','210334_x_at','201795_at','38158_at','208232_x_at','201307_at','226742_at','205046_at','204026_s_at','226218_at','217824_at','233399_x_at','224009_x_at','215177_s_at','202532_s_at','238662_at','212788_x_at','220351_at','202542_s_at','243018_at','209683_at','212282_at','208967_s_at','225366_at','217852_s_at','225601_at','231210_at','214482_at','208942_s_at','219550_at','231989_s_at','202553_s_at','223811_s_at','221041_s_at','221677_s_at','213350_at','200775_s_at','226217_at','217728_at','201930_at','216473_x_at','211714_x_at','221755_at','AFFX-HUMISGF3A/M97935_MA_at','206204_at','217548_at','215181_at','217732_s_at','214612_x_at','202813_at','200875_s_at','201292_at','222680_s_at','233437_at','223381_at','209026_x_at','221606_s_at','231738_at','230034_x_at','213007_at','242180_at','202322_s_at','208904_s_at','214150_x_at','238116_at','208732_at','200701_at','208667_s_at','208747_s_at','218662_s_at','211963_s_at','201555_at','207618_s_at','200933_x_at','221826_at','218355_at','219510_at','218365_s_at','222713_s_at','222154_s_at','228416_at','201102_s_at','203145_at','238780_s_at','202884_s_at','201398_s_at','212055_at','202107_s_at')

coefs_s92 <- data.frame(probes=names(coefficients), coefs=coefficients)  

## GEP70 https://github.com/bswhite/Celgene-Multiple-Myeloma-Challenge-Baseline-Models/blob/master/publishedClassifiers/uams-70/uams-70.R
up<-c("202345_s_at","1555864_s_at","204033_at","206513_at","1555274_a_at","211576_s_at","204016_at","1565951_s_at","219918_s_at","201947_s_at","213535_s_at","204092_s_at","213607_x_at","208117_s_at","210334_x_at","204023_at","201897_s_at","216194_s_at","225834_at","238952_x_at","200634_at","208931_s_at","206332_s_at","220789_s_at","218947_s_at","213310_at","224523_s_at","201231_s_at","217901_at","226936_at","58696_at","200916_at","201614_s_at","200966_x_at","225082_at","242488_at","243011_at","201105_at","224200_s_at","222417_s_at","210460_s_at","200750_s_at","206364_at","201091_s_at","203432_at","221970_s_at","212533_at","213194_at","244686_at","200638_s_at","205235_s_at")
down<-c("201921_at","227278_at","209740_s_at","227547_at","225582_at","200850_s_at","213628_at","209717_at","222495_at","1557277_a_at","1554736_at","218924_s_at","226954_at","202838_at","230192_at","48106_at","237964_at","202729_s_at","212435_at")
up.coeffs <- rep(1.0, length(up)) / length(up)
down.coeffs <- rep(-1.0, length(down)) / length(down)
coefficients <- c(up.coeffs, down.coeffs)
names(coefficients) <- c(up, down)
coefs_gep70 <- data.frame(probes=names(coefficients), coefs=coefficients)



